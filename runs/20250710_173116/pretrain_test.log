/opt/conda/lib/python3.10/site-packages/lightning_fabric/__init__.py:40: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
Warning: Could not import grit_layer: No module named 'opt_einsum'
./datasets/ZINC
[*] Loaded dataset 'subset' from 'PyG-ZINC':
/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
  Data(x=[277864, 1], edge_index=[2, 597970], edge_attr=[597970], y=[12000])
  undirected: True
  num graphs: 12000
/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
  avg num_nodes/graph: 23
  num node features: 1
  num edge features: 1
  num classes: (appears to be a regression task)
Precomputing Positional Encoding statistics: ['RRWP'] for all graphs...
  ...estimated to be undirected: True
  0%|          | 0/12000 [00:00<?, ?it/s]100%|██████████| 12000/12000 [00:07<00:00, 1508.76it/s]
Done! Took 00:00:08.73
Adding virtual node and edges
  0%|          | 0/12000 [00:00<?, ?it/s]100%|██████████| 12000/12000 [00:06<00:00, 1728.87it/s]
Preprocessing PE
  0%|          | 0/12000 [00:00<?, ?it/s]100%|██████████| 12000/12000 [00:03<00:00, 3573.68it/s]
[*] Run ID 0: seed=0, split_index=0
    Starting now: 2025-07-10 17:32:15.860396
/opt/conda/lib/python3.10/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
GraphGymModule_custom(
  (model): GraphTransformerEncoder(
    (act): ReLU()
    (node_emb): TypeDictNodeEncoder(
      (encoder): Embedding(27, 36, padding_idx=0)
    )
    (edge_emb): TypeDictEdgeEncoder(
      (encoder): Embedding(14, 36, padding_idx=0)
    )
    (prefix_emb): Embedding(1, 64, padding_idx=0)
    (label_embed_regression): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=64, bias=True)
      (3): Identity()
      (4): ReLU()
    )
    (label_embed_classification): Embedding(3, 64)
    (posenc_emb): Sequential(
      (0): Identity()
      (1): Linear(in_features=20, out_features=28, bias=True)
    )
    (posenc_emb_edge): Sequential(
      (0): Identity()
      (1): Linear(in_features=20, out_features=28, bias=True)
    )
    (node_in_mlp): Sequential(
      (0): Linear(in_features=64, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (edge_in_mlp): Sequential(
      (0): Linear(in_features=64, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (batch_norm_in_h): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (batch_norm_in_e): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (GTE_layers): ModuleList(
      (0-9): 10 x GraphTransformerEncoderLayer(in_channels=64, out_channels=64, heads=4, residual=True)
      [GraphTransformerEncoderLayer(
        (act): ReLU()
        (attention): GTE_Attention(
          (dropout): Dropout(p=0.5, inplace=False)
          (Q): Linear(in_features=64, out_features=64, bias=True)
          (K): Linear(in_features=64, out_features=64, bias=False)
          (E): Linear(in_features=64, out_features=128, bias=True)
          (V): Linear(in_features=64, out_features=64, bias=False)
          (act): ReLU()
        )
        (mpnn): GINE(
          (dropout): Dropout(p=0.0, inplace=False)
          (act): ReLU()
          (lin_edge): Linear(in_features=64, out_features=64, bias=True)
          (mlp): Sequential(
            (0): Linear(in_features=64, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=64, bias=True)
          )
        )
        (temb_proj_h): Linear(in_features=0, out_features=64, bias=True)
        (temb_proj_e): Linear(in_features=0, out_features=64, bias=True)
        (O_h): Linear(in_features=64, out_features=64, bias=True)
        (O_e): Linear(in_features=64, out_features=64, bias=True)
        (batch_norm1_h): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (batch_norm1_e): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (FFN_h_layer1): Linear(in_features=64, out_features=128, bias=True)
        (FFN_h_layer2): Linear(in_features=128, out_features=64, bias=True)
        (FFN_e_layer1): Linear(in_features=64, out_features=128, bias=True)
        (FFN_e_layer2): Linear(in_features=128, out_features=64, bias=True)
        (batch_norm2_h): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (batch_norm2_e): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )]
    )
    (final_layer_node): Sequential(
      (0): Linear(in_features=64, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
    )
    (final_layer_edge): Sequential(
      (0): Linear(in_features=64, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
    )
    (graph_out_mlp): Identity()
    (graph_out_mlp_2): Identity()
    (decode_node): Linear(in_features=4, out_features=31, bias=True)
    (decode_edge): Linear(in_features=4, out_features=14, bias=True)
    (decode_graph): Linear(in_features=4, out_features=1, bias=True)
  )
)
accelerator: cuda:0
benchmark: False
bn:
  eps: 1e-05
  mom: 0.1
cfg_dest: config.yaml
cond:
  attn:
    O_e: True
    act: relu
    attn_dropout: 0.0
    batch_norm: True
    bn_momentum: 0.1
    bn_no_runner: False
    clamp: 5.0
    deg_scaler: False
    edge_enhance: True
    full_attn: True
    layer_norm: False
    norm_e: True
    sparse: False
    use: True
    use_bias: False
  batch_norm: True
  bn_momentum: 0.1
  bn_no_runner: False
  edge_encoder_name: TypeDictEdge
  layer_norm: False
  node_encoder_name: TypeDictNode
custom_metrics: []
dataset:
  add_virtual_node_edge: True
  cache_load: False
  cache_save: False
  dir: ./datasets
  edge_dim: 128
  edge_encoder: True
  edge_encoder_bn: False
  edge_encoder_name: TypeDictEdge
  edge_encoder_num_types: 4
  edge_message_ratio: 0.8
  edge_negative_sampling_ratio: 1.0
  edge_train_mode: all
  encoder: True
  encoder_bn: True
  encoder_dim: 128
  encoder_name: db
  format: PyG-ZINC
  label_column: none
  label_table: none
  location: local
  name: subset
  node_encoder: True
  node_encoder_bn: False
  node_encoder_name: TypeDictNode
  node_encoder_num_types: 21
  num_hop: 0
  remove_feature: False
  resample_disjoint: False
  resample_negative: False
  shuffle_split: True
  slic_compactness: 10
  split: [0.8, 0.1, 0.1]
  split_dir: ./splits
  split_index: 0
  split_mode: standard
  subgraph: False
  task: graph
  task_type: regression
  to_undirected: False
  transductive: False
  transform: none
  tu_simple: True
devices: 1
diffusion:
  conditioning_key: crossattn
  edge_factor: 1.0
  graph_factor: 0.0
dt:
  attn:
    O_e: True
    act: relu
    attn_dropout: 0.0
    batch_norm: True
    bn_momentum: 0.1
    bn_no_runner: False
    clamp: 5.0
    deg_scaler: False
    edge_enhance: True
    full_attn: True
    layer_norm: False
    norm_e: True
    sparse: False
    use: True
    use_bias: False
  batch_norm: True
  bn_momentum: 0.1
  bn_no_runner: False
  layer_norm: False
encoder:
  O_e: True
  act: relu
  attn:
    O_e: True
    act: relu
    attn_dropout: 0.0
    attn_product: mul
    attn_reweight: True
    batch_norm: True
    bn_momentum: 0.1
    bn_no_runner: False
    clamp: 5.0
    deg_scaler: False
    edge_enhance: True
    edge_reweight: True
    full_attn: True
    fwl: False
    layer_norm: False
    norm_e: True
    score_act: True
    signed_sqrt: True
    sparse: False
    use: True
    use_bias: False
  attn_dropout: 0.5
  batch_norm: True
  bn_momentum: 0.1
  bn_no_runner: False
  dropout: 0.0
  edge_encoder: True
  edge_encoder_bn: False
  edge_encoder_name: TypeDictEdge
  edge_encoder_num_types: 4
  ff_e: False
  hid_dim: 64
  in_dim: 36
  label_embed_type: add_virtual
  layer_norm: False
  mpnn:
    act: relu
    dropout: 0.0
    edge_enhance: True
    enable: True
    project_edge: True
  node_encoder: True
  node_encoder_bn: False
  node_encoder_name: TypeDictNode
  node_encoder_num_types: 21
  norm_e: True
  num_heads: 4
  num_layers: 10
  num_task: 1
  out_dim: 4
  pe_raw_norm: None
  pool: add
  pool_edge: True
  pool_vn: False
  posenc_dim: 28
  posenc_in_dim: 20
  posenc_in_dim_edge: 20
  prefix_dim: 64
  prefix_type: add_virtual
  residual: True
  task_type: regression
  temb_dim: 0
  update_e: True
  use_time: False
example_arg: example
example_group:
  example_arg: example
gnn:
  act: relu
  agg: mean
  att_final_linear: False
  att_final_linear_bn: False
  att_heads: 1
  batchnorm: True
  clear_feature: True
  dim_inner: 64
  dropout: 0.0
  head: san_graph
  keep_edge: 0.5
  l2norm: True
  layer_type: generalconv
  layers_mp: 2
  layers_post_mp: 3
  layers_pre_mp: 0
  msg_direction: single
  normalize_adj: False
  residual: False
  self_msg: concat
  skip_every: 1
  stage_type: stack
gpu_mem: False
gt:
  attn:
    O_e: True
    act: relu
    clamp: 5.0
    deg_scaler: True
    edge_enhance: True
    full_attn: True
    fwl: False
    norm_e: True
    sparse: False
    use: True
    use_bias: False
  attn_dropout: 0.2
  batch_norm: True
  bigbird:
    add_cross_attention: False
    attention_type: block_sparse
    block_size: 3
    chunk_size_feed_forward: 0
    hidden_act: relu
    is_decoder: False
    layer_norm_eps: 1e-06
    max_position_embeddings: 128
    num_random_blocks: 3
    use_bias: False
  bn_momentum: 0.1
  bn_no_runner: False
  dim_hidden: 64
  dropout: 0.0
  full_graph: True
  gamma: 1e-05
  layer_norm: False
  layer_type: GritTransformer
  layers: 10
  n_heads: 8
  pna_degrees: []
  residual: True
  update_e: True
mem:
  inplace: False
metric_agg: argmin
metric_best: mae
mlflow:
  name: zinc-GRIT-RRWP
  project: Exp
  use: False
model:
  edge_decoding: dot
  graph_pooling: add
  loss_fun: l1
  match_upper: True
  size_average: mean
  thresh: 0.5
  type: GraphTransformerEncoder
name_tag: 
num_threads: 6
num_workers: 0
optim:
  base_lr: 0.001
  batch_accumulation: 1
  clip_grad_norm: True
  lr_decay: 0.1
  max_epoch: 1
  min_lr: 1e-06
  momentum: 0.9
  num_warmup_epochs: 50
  optimizer: adamW
  reduce_factor: 0.1
  schedule_patience: 10
  scheduler: cosine_with_warmup
  steps: [30, 60, 90]
  weight_decay: 1e-05
out_dir: results/zinc-encoder
posenc_ERE:
  accuracy: 0.1
  dim_pe: 16
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_ERN:
  accuracy: 0.1
  dim_pe: 16
  enable: False
  er_dim: none
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_EdgeRWSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_ElstaticSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: range(10)
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_EquivStableLapPE:
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  raw_norm_type: none
posenc_HKdiagSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_HodgeLap1PE:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_InterRWSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_LapPE:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_RD:
  dim_pe: 16
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_RRWP:
  add_identity: True
  dim_pe: 16
  enable: True
  ksteps: 20
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
  real_emb: True
  spd: False
posenc_RWSE:
  dim_pe: 28
  enable: False
  kernel:
    times: []
    times_func: range(1,21)
  layers: 3
  local: False
  model: Linear
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: BatchNorm
posenc_SignNet:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  phi_hidden_dim: 64
  phi_out_dim: 4
  post_layers: 0
  raw_norm_type: none
prep:
  add_edge_index: True
  add_reverse_edges: True
  add_self_loops: False
  dist_cutoff: 510
  dist_enable: False
  exp: False
  exp_algorithm: Random-d
  exp_count: 1
  exp_deg: 5
  exp_max_num_iters: 100
  layer_edge_indices_dir: None
  num_virt_node: 0
  train_percent: 0.6
  use_exp_edges: True
pretrained:
  dir: 
  freeze_main: False
  reset_prediction_head: True
print: both
regression_model:
  encoder:
    attn:
      O_e: True
      act: relu
      attn_dropout: 0.0
      batch_norm: True
      bn_momentum: 0.1
      bn_no_runner: False
      clamp: 5.0
      deg_scaler: False
      edge_enhance: True
      full_attn: True
      layer_norm: False
      norm_e: True
      sparse: False
      use: True
      use_bias: False
    batch_norm: True
    bn_momentum: 0.1
    bn_no_runner: False
    edge_encoder_name: TypeDictEdge
    layer_norm: False
    node_encoder_name: TypeDictNode
round: 5
run_dir: results/zinc-encoder/0
run_id: 0
run_multiple_splits: []
seed: 0
share:
  dim_in: 1
  dim_out: 1
  num_splits: 3
tensorboard_agg: True
tensorboard_each_run: False
train:
  auto_resume: False
  batch_size: 64
  ckpt_best: True
  ckpt_clean: False
  ckpt_period: 25
  enable_ckpt: True
  ensemble_mode: none
  ensemble_repeat: 1
  epoch_resume: -1
  eval_period: 1
  iter_per_epoch: 32
  mode: pretrain_encoder
  neighbor_sizes: [20, 15, 10, 5]
  node_per_graph: 32
  pretrain:
    atom_bond_only: True
    edge_factor: 0.0
    graph_factor: 1.0
    input_target: True
    mask_edge_prob: 0.0
    mask_label_prob: 0.5
    mask_node_prob: 0.0
    node_factor: 0.0
    original_task: True
    recon: none
  radius: extend
  sample_node: False
  sampler: full_batch
  skip_train_eval: False
  start_eval_epoch: -1
  walk_length: 4
val:
  node_per_graph: 32
  radius: extend
  sample_node: False
  sampler: full_batch
view_emb: False
wandb:
  entity: gtransformers
  name: 
  project: ZINC
  use: False
Num parameters: 892314
Start from epoch 0
tensor([1, 1, 2, 3, 1, 1, 1, 1, 3, 1, 2, 1, 3, 3, 1, 1, 1, 1, 1, 1],
       device='cuda:0')
tensor([[-0.3723, -0.1422, -0.1432,  0.2750],
        [-0.4896,  0.0831, -0.4839,  0.5013],
        [-0.4808,  0.0591, -0.6094,  0.1500],
        [-0.3402, -0.0136, -0.3076, -0.0825],
        [-0.3881, -0.0092, -0.3358,  0.4422],
        [-0.2617, -0.0767, -0.2541,  0.2834],
        [-0.3254, -0.0295, -0.3157,  0.2807],
        [-0.4881,  0.0963, -0.5761,  0.3093],
        [-0.2531, -0.0116, -0.3636,  0.1796],
        [-0.4960,  0.1659, -0.6560,  0.3961],
        [-0.4426,  0.1037, -0.6864,  0.1725],
        [-0.4477,  0.0887, -0.4955,  0.4296],
        [-0.4713,  0.0085, -0.2882,  0.4874],
        [-0.3259,  0.0289, -0.4603,  0.4834],
        [-0.6280,  0.0849, -0.3872,  0.4267],
        [-0.4724,  0.0283, -0.3133,  0.3232],
        [-0.5024, -0.0235, -0.2470,  0.3611],
        [-0.4845, -0.0935, -0.1628,  0.3124],
        [-0.5465, -0.0530, -0.1850,  0.3298],
        [-0.4660, -0.0071, -0.2612,  0.3218]], device='cuda:0',
       grad_fn=<SliceBackward0>)
tensor([8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       device='cuda:0')
tensor([[-0.1156, -0.3375, -0.5080, -0.4436],
        [ 0.8042, -0.0736, -1.0805, -1.1400],
        [-0.3029, -0.1117, -0.4055, -0.1493],
        [-0.1651, -0.2473, -0.4677, -0.0982],
        [-0.3364, -0.3413, -0.1750, -0.0418],
        [-0.2396, -0.1788, -0.3290, -0.0306],
        [-0.2949, -0.1895, -0.2374, -0.0427],
        [-0.2655, -0.2285, -0.2364,  0.0172],
        [-0.2953, -0.2115, -0.2894, -0.0819],
        [-0.2623, -0.2334, -0.2601,  0.0072],
        [-0.3742, -0.3118, -0.1928, -0.1218],
        [-0.3252, -0.2279, -0.2930, -0.0599],
        [-0.3976, -0.1702, -0.3124, -0.1604],
        [-0.3853, -0.1529, -0.3163, -0.1501],
        [-0.3414, -0.2242, -0.2441, -0.0337],
        [-0.3413, -0.2794, -0.2071, -0.0562],
        [-0.2520, -0.2061, -0.2522,  0.0159],
        [-0.3091, -0.2665, -0.2313, -0.0357],
        [-0.3055, -0.2825, -0.1974,  0.0108],
        [-0.3032, -0.2541, -0.2232, -0.0234]], device='cuda:0',
       grad_fn=<SliceBackward0>)
train: {'epoch': 0, 'time_epoch': 27.10185, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 304.06041201, 'lr': 0.0, 'params': 892314, 'time_iter': 0.17262, 'mae': 304.06039, 'r2': -25244.27948, 'spearmanr': -0.24977, 'mse': 102076.6875, 'rmse': 319.49442}
...computing epoch stats took: 0.01s
val: {'epoch': 0, 'time_epoch': 0.89425, 'loss': 300.27558813, 'lr': 0, 'params': 892314, 'time_iter': 0.05589, 'mae': 300.71823, 'r2': -25342.5964, 'spearmanr': -0.23433, 'mse': 99822.22656, 'rmse': 315.94656, 'loss_recon': 0.0, 'loss_recon_node': 0.0, 'loss_recon_edge': 0.0, 'loss_recon_tuple': 0.0, 'loss_recon_nodepe': 0.0, 'loss_recon_edgepe': 0.0}
...computing epoch stats took: 0.00s
test: {'epoch': 0, 'time_epoch': 1.09634, 'loss': 300.9747605, 'lr': 0, 'params': 892314, 'time_iter': 0.06852, 'mae': 301.43079, 'r2': -24602.13896, 'spearmanr': -0.2472, 'mse': 100092.9375, 'rmse': 316.37466, 'loss_recon': 0.0, 'loss_recon_node': 0.0, 'loss_recon_edge': 0.0, 'loss_recon_tuple': 0.0, 'loss_recon_nodepe': 0.0, 'loss_recon_edgepe': 0.0}
...computing epoch stats took: 0.00s
> Epoch 0: took 29.1s (avg 29.1s) | Best so far: epoch 0	train_loss: 304.0604 train_mae: 304.0604	val_loss: 300.2756 val_mae: 300.7182	test_loss: 300.9748 test_mae: 301.4308
Avg time per epoch: 29.12s
Total train loop time: 0.01h
Task done, results saved in results/zinc-encoder/0
0
{'epoch': 0, 'time_epoch': 1.09634, 'loss': 300.9747605, 'lr': 0, 'params': 892314, 'time_iter': 0.06852, 'mae': 301.43079, 'r2': -24602.13896, 'spearmanr': -0.2472, 'mse': 100092.9375, 'rmse': 316.37466, 'loss_recon': 0.0, 'loss_recon_node': 0.0, 'loss_recon_edge': 0.0, 'loss_recon_tuple': 0.0, 'loss_recon_nodepe': 0.0, 'loss_recon_edgepe': 0.0}
{'epoch': 0, 'time_epoch': 0.89425, 'loss': 300.27558813, 'lr': 0, 'params': 892314, 'time_iter': 0.05589, 'mae': 300.71823, 'r2': -25342.5964, 'spearmanr': -0.23433, 'mse': 99822.22656, 'rmse': 315.94656, 'loss_recon': 0.0, 'loss_recon_node': 0.0, 'loss_recon_edge': 0.0, 'loss_recon_tuple': 0.0, 'loss_recon_nodepe': 0.0, 'loss_recon_edgepe': 0.0}
{'epoch': 0, 'time_epoch': 27.10185, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 304.06041201, 'lr': 0.0, 'params': 892314, 'time_iter': 0.17262, 'mae': 304.06039, 'r2': -25244.27948, 'spearmanr': -0.24977, 'mse': 102076.6875, 'rmse': 319.49442}
Results aggregated across runs saved in results/zinc-encoder/agg
[*] All done: 2025-07-10 17:32:51.668560
